{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Working with functions in Azure OpenAI\n",
        "This notebook shows how to use the Chat Completions API in combination with functions to extend the current capabilities of GPT models. GPT models, do not inherently support real-time interaction with external systems, databases, or files. However, functions can be used to do so.\n",
        "\n",
        "Overview: <br>\n",
        "`functions` is an optional parameter in the Chat Completion API which can be used to provide function specifications. This allows models to generate function arguments for the specifications provided by the user. \n",
        "\n",
        "Note: The API will not execute any function calls. Executing function calls using the outputed argments must be done by developers. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# if needed, install and/or upgrade to the latest version of the OpenAI Python library\n",
        "%pip install --upgrade openai"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: openai in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (0.27.8)\nRequirement already satisfied: requests>=2.20 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai) (2.28.2)\nRequirement already satisfied: tqdm in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai) (4.64.1)\nRequirement already satisfied: aiohttp in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai) (3.8.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.20->openai) (3.0.1)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.20->openai) (2022.6.15)\nRequirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->openai) (22.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->openai) (1.4.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/anaconda/envs/azureml_py310_sdkv2/bin/python -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1692252561345
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Set up Azure OpenAI\n",
        "load_dotenv(\"credentials.env\")\n",
        "\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\") # Api base is the 'Endpoint' which can be found in Azure Portal where Azure OpenAI is created. It looks like https://xxxxxx.openai.azure.com/\n",
        "openai.api_version =\"2023-07-01-preview\"\n",
        "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1692253303466
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## 1.0 Test functions\n",
        "\n",
        "This code calls the model with the user query and the set of functions defined in the functions parameter. The model then can choose if it calls a function. If a function is called, the content will be in a strigified JSON object. The function call that should be made and arguments are location in:  response[`choices`][0][`function_call`]."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "deployment_id=\"gpt-4-32k\""
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1692253317666
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_function_call(messages, function_call = \"auto\"):\n",
        "    # Define the functions to use\n",
        "    functions = [\n",
        "        {\n",
        "            \"name\": \"get_current_weather\",\n",
        "            \"description\": \"Get the current weather in a given location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                    },\n",
        "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
        "                },\n",
        "                \"required\": [\"location\"],\n",
        "            },\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # Call the model with the user query (messages) and the functions defined in the functions parameter\n",
        "    response = openai.ChatCompletion.create(\n",
        "        engine = deployment_id,\n",
        "        messages=messages,\n",
        "        functions=functions,\n",
        "        function_call=function_call, \n",
        "    )\n",
        "\n",
        "    return response"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1692253318328
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forcing the use of a specific function or no function\n",
        "By changing the value of the `functions` parameter you can allow the model to decide what function to use, force the model to use a specific function, or force the model to use no function."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "first_message = [{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}]\n",
        "# 'auto' : Let the model decide what function to call\n",
        "print(\"Let the model decide what function to call:\")\n",
        "print (get_function_call(first_message, \"auto\"))\n",
        "\n",
        "# 'none' : Don't call any function \n",
        "print(\"Don't call any function:\")\n",
        "print (get_function_call(first_message, \"none\"))\n",
        "\n",
        "# force a specific function call\n",
        "print(\"Force a specific function call:\")\n",
        "print (get_function_call(first_message, function_call={\"name\": \"get_current_weather\"}))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Let the model decide what function to call:\n{\n  \"id\": \"chatcmpl-7oQgKcJmASLzUbq73GXpsFO1QJoFZ\",\n  \"object\": \"chat.completion\",\n  \"created\": 1692253320,\n  \"model\": \"gpt-4-32k\",\n  \"prompt_annotations\": [\n    {\n      \"prompt_index\": 0,\n      \"content_filter_results\": {\n        \"hate\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"self_harm\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"sexual\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"violence\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        }\n      }\n    }\n  ],\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"finish_reason\": \"function_call\",\n      \"message\": {\n        \"role\": \"assistant\",\n        \"function_call\": {\n          \"name\": \"get_current_weather\",\n          \"arguments\": \"{\\n  \\\"location\\\": \\\"San Francisco\\\"\\n}\"\n        }\n      },\n      \"content_filter_results\": {}\n    }\n  ],\n  \"usage\": {\n    \"completion_tokens\": 17,\n    \"prompt_tokens\": 83,\n    \"total_tokens\": 100\n  }\n}\nDon't call any function:\n{\n  \"id\": \"chatcmpl-7oQgNyhsGFtA2ljCjWSugHmnioD4h\",\n  \"object\": \"chat.completion\",\n  \"created\": 1692253323,\n  \"model\": \"gpt-4-32k\",\n  \"prompt_annotations\": [\n    {\n      \"prompt_index\": 0,\n      \"content_filter_results\": {\n        \"hate\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"self_harm\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"sexual\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"violence\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        }\n      }\n    }\n  ],\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"finish_reason\": \"stop\",\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Sure, let me get this information for you.\"\n      },\n      \"content_filter_results\": {\n        \"hate\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"self_harm\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"sexual\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"violence\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        }\n      }\n    }\n  ],\n  \"usage\": {\n    \"completion_tokens\": 10,\n    \"prompt_tokens\": 84,\n    \"total_tokens\": 94\n  }\n}\nForce a specific function call:\n{\n  \"id\": \"chatcmpl-7oQgQqe2LICgqi3gdXqhtlSv6ZFJt\",\n  \"object\": \"chat.completion\",\n  \"created\": 1692253326,\n  \"model\": \"gpt-4-32k\",\n  \"prompt_annotations\": [\n    {\n      \"prompt_index\": 0,\n      \"content_filter_results\": {\n        \"hate\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"self_harm\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"sexual\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"violence\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        }\n      }\n    }\n  ],\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"finish_reason\": \"stop\",\n      \"message\": {\n        \"role\": \"assistant\",\n        \"function_call\": {\n          \"name\": \"get_current_weather\",\n          \"arguments\": \"{\\n  \\\"location\\\": \\\"San Francisco\\\"\\n}\"\n        }\n      },\n      \"content_filter_results\": {}\n    }\n  ],\n  \"usage\": {\n    \"completion_tokens\": 10,\n    \"prompt_tokens\": 90,\n    \"total_tokens\": 100\n  }\n}\n"
        }
      ],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1692253327555
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## 2.0 Defining functions\n",
        "Now that we know how to work with functions, let's define some functions in code so that we can walk through the process of using functions end to end."
      ],
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "### Function #1: Get current time"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pytz\n",
        "from datetime import datetime\n",
        "\n",
        "def get_current_time(location):\n",
        "    try:\n",
        "        # Get the timezone for the city\n",
        "        timezone = pytz.timezone(location)\n",
        "\n",
        "        # Get the current time in the timezone\n",
        "        now = datetime.now(timezone)\n",
        "        current_time = now.strftime(\"%I:%M:%S %p\")\n",
        "\n",
        "        return current_time\n",
        "    except:\n",
        "        return \"Sorry, I couldn't find the timezone for that location.\""
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1692253492521
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_current_time(\"America/New_York\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 27,
          "data": {
            "text/plain": "'02:23:35 AM'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1692253415525
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## 3.0 Calling a function using GPT\n",
        "\n",
        "Steps for Function Calling: \n",
        "\n",
        "1. Call the model with the user query and a set of functions defined in the functions parameter.\n",
        "2. The model can choose to call a function; if so, the content will be a stringified JSON object adhering to your custom schema (note: the model may generate invalid JSON or hallucinate parameters).\n",
        "3. Parse the string into JSON in your code, and call your function with the provided arguments if they exist.\n",
        "4. Call the model again by appending the function response as a new message, and let the model summarize the results back to the user.\n",
        "\n",
        "### 3.1 Describe the functions so that the model knows how to call them"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "functions = [\n",
        "        {\n",
        "            \"name\": \"get_current_time\",\n",
        "            \"description\": \"Get the current time in a given location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The location name. The pytz is used to get the timezone for that location. Location names should be in a format like America/New_York, Asia/Bangkok, Europe/London\",\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"location\"],\n",
        "            },\n",
        "        },\n",
        "        \n",
        "    ]\n",
        "\n",
        "available_functions = {\n",
        "            \"get_current_time\": get_current_time\n",
        "        } "
      ],
      "outputs": [],
      "execution_count": 40,
      "metadata": {
        "gather": {
          "logged": 1692253728312
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Define a helper function to validate the function call\n",
        "It's possible that the models could generate incorrect function calls so it's important to validate the calls. Here we define a simple helper function to validate the function call although you could apply more complex validation for your use case."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "\n",
        "# helper method used to check if the correct arguments are provided to a function\n",
        "def check_args(function, args):\n",
        "    sig = inspect.signature(function)\n",
        "    params = sig.parameters\n",
        "\n",
        "    # Check if there are extra arguments\n",
        "    for name in args:\n",
        "        if name not in params:\n",
        "            return False\n",
        "    # Check if the required arguments are provided \n",
        "    for name, param in params.items():\n",
        "        if param.default is param.empty and name not in args:\n",
        "            return False\n",
        "\n",
        "    return True"
      ],
      "outputs": [],
      "execution_count": 34,
      "metadata": {
        "gather": {
          "logged": 1692253516655
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1692253597802
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_conversation(messages, functions, available_functions, deployment_id):\n",
        "    # Step 1: send the conversation and available functions to GPT\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        deployment_id=deployment_id,\n",
        "        messages=messages,\n",
        "        functions=functions,\n",
        "        function_call=\"auto\", \n",
        "    )\n",
        "    response_message = response[\"choices\"][0][\"message\"]\n",
        "\n",
        "\n",
        "    # Step 2: check if GPT wanted to call a function\n",
        "    if response_message.get(\"function_call\"):\n",
        "        print(\"Recommended Function call:\")\n",
        "        print(response_message.get(\"function_call\"))\n",
        "        print()\n",
        "        \n",
        "        # Step 3: call the function\n",
        "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
        "        \n",
        "        function_name = response_message[\"function_call\"][\"name\"]\n",
        "        \n",
        "        # verify function exists\n",
        "        if function_name not in available_functions:\n",
        "            return \"Function \" + function_name + \" does not exist\"\n",
        "        function_to_call = available_functions[function_name]  \n",
        "        \n",
        "        # verify function has correct number of arguments\n",
        "        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
        "        if check_args(function_to_call, function_args) is False:\n",
        "            return \"Invalid number of arguments for function: \" + function_name\n",
        "        function_response = function_to_call(**function_args)\n",
        "        \n",
        "        print(\"Output of function call:\")\n",
        "        print(function_response)\n",
        "        print()\n",
        "        \n",
        "        # Step 4: send the info on the function call and function response to GPT\n",
        "        \n",
        "        # adding assistant response to messages\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": response_message[\"role\"],\n",
        "                \"name\": response_message[\"function_call\"][\"name\"],\n",
        "                \"content\": response_message[\"function_call\"][\"arguments\"],\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # adding function response to messages\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"function\",\n",
        "                \"name\": function_name,\n",
        "                \"content\": function_response,\n",
        "            }\n",
        "        )  # extend conversation with function response\n",
        "\n",
        "        print(\"Messages in second request:\")\n",
        "        for message in messages:\n",
        "            print(message)\n",
        "        print()\n",
        "\n",
        "        second_response = openai.ChatCompletion.create(\n",
        "            messages=messages,\n",
        "            deployment_id=deployment_id\n",
        "        )  # get a new response from GPT where it can see the function response\n",
        "\n",
        "        return second_response"
      ],
      "outputs": [],
      "execution_count": 41,
      "metadata": {
        "gather": {
          "logged": 1692253736417
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{\"role\": \"user\", \"content\": \"What time is it in New York?\"}]\n",
        "assistant_response = run_conversation(messages, functions, available_functions, deployment_id)\n",
        "print(assistant_response['choices'][0]['message'])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Recommended Function call:\n{\n  \"name\": \"get_current_time\",\n  \"arguments\": \"{\\n  \\\"location\\\": \\\"America/New_York\\\"\\n}\"\n}\n\nOutput of function call:\n02:28:59 AM\n\nMessages in second request:\n{'role': 'user', 'content': 'What time is it in New York?'}\n{'role': 'assistant', 'name': 'get_current_time', 'content': '{\\n  \"location\": \"America/New_York\"\\n}'}\n{'role': 'function', 'name': 'get_current_time', 'content': '02:28:59 AM'}\n\n{\n  \"role\": \"assistant\",\n  \"content\": \"It is currently 2:28 AM in New York.\"\n}\n"
        }
      ],
      "execution_count": 42,
      "metadata": {
        "gather": {
          "logged": 1692253741266
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 4,
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}