{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Set up Azure OpenAI\n",
        "load_dotenv(\"credentials.env\")\n",
        "\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\") # Api base is the 'Endpoint' which can be found in Azure Portal where Azure OpenAI is created. It looks like https://xxxxxx.openai.azure.com/\n",
        "openai.api_version =\"2023-07-01-preview\"\n",
        "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1692715529402
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Test Function Calling"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_function_call(messages, function_call = \"auto\"):\n",
        "    # Define the functions to use\n",
        "    functions = [\n",
        "        {\n",
        "            \"name\": \"get_current_weather\",\n",
        "            \"description\": \"Get the current weather in a given location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                    },\n",
        "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
        "                },\n",
        "                \"required\": [\"location\"],\n",
        "            },\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # Call the model with the user query (messages) and the functions defined in the functions parameter\n",
        "    response = openai.ChatCompletion.create(\n",
        "        deployment_id = \"gpt-4-32k\",\n",
        "        messages=messages,\n",
        "        functions=functions,\n",
        "        function_call=function_call, \n",
        "    )\n",
        "\n",
        "    return response"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1692715309784
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_message = [{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}]\n",
        "# 'auto' : Let the model decide what function to call\n",
        "print(\"Let the model decide what function to call:\")\n",
        "print (get_function_call(first_message, \"auto\"))\n",
        "\n",
        "# 'none' : Don't call any function \n",
        "print(\"Don't call any function:\")\n",
        "print (get_function_call(first_message, \"none\"))\n",
        "\n",
        "# force a specific function call\n",
        "print(\"Force a specific function call:\")\n",
        "print (get_function_call(first_message, function_call={\"name\": \"get_current_weather\"}))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Let the model decide what function to call:\n{\n  \"id\": \"chatcmpl-7qMroPeuIqx5vpxejFYejut2VZQyL\",\n  \"object\": \"chat.completion\",\n  \"created\": 1692715312,\n  \"model\": \"gpt-4-32k\",\n  \"prompt_annotations\": [\n    {\n      \"prompt_index\": 0,\n      \"content_filter_results\": {\n        \"hate\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"self_harm\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"sexual\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"violence\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        }\n      }\n    }\n  ],\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"finish_reason\": \"function_call\",\n      \"message\": {\n        \"role\": \"assistant\",\n        \"function_call\": {\n          \"name\": \"get_current_weather\",\n          \"arguments\": \"{\\n  \\\"location\\\": \\\"San Francisco\\\"\\n}\"\n        }\n      },\n      \"content_filter_results\": {}\n    }\n  ],\n  \"usage\": {\n    \"completion_tokens\": 17,\n    \"prompt_tokens\": 83,\n    \"total_tokens\": 100\n  }\n}\nDon't call any function:\n{\n  \"id\": \"chatcmpl-7qMrq7vOxnaHv5E6hytcQ2JUhtSlL\",\n  \"object\": \"chat.completion\",\n  \"created\": 1692715314,\n  \"model\": \"gpt-4-32k\",\n  \"prompt_annotations\": [\n    {\n      \"prompt_index\": 0,\n      \"content_filter_results\": {\n        \"hate\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"self_harm\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"sexual\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"violence\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        }\n      }\n    }\n  ],\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"finish_reason\": \"stop\",\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Sure, let me check that for you.\"\n      },\n      \"content_filter_results\": {\n        \"hate\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"self_harm\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"sexual\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"violence\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        }\n      }\n    }\n  ],\n  \"usage\": {\n    \"completion_tokens\": 9,\n    \"prompt_tokens\": 84,\n    \"total_tokens\": 93\n  }\n}\nForce a specific function call:\n{\n  \"id\": \"chatcmpl-7qMrrEJqhHYW6akTIbEMuN9xAMu67\",\n  \"object\": \"chat.completion\",\n  \"created\": 1692715315,\n  \"model\": \"gpt-4-32k\",\n  \"prompt_annotations\": [\n    {\n      \"prompt_index\": 0,\n      \"content_filter_results\": {\n        \"hate\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"self_harm\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"sexual\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        },\n        \"violence\": {\n          \"filtered\": false,\n          \"severity\": \"safe\"\n        }\n      }\n    }\n  ],\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"finish_reason\": \"stop\",\n      \"message\": {\n        \"role\": \"assistant\",\n        \"function_call\": {\n          \"name\": \"get_current_weather\",\n          \"arguments\": \"{\\n  \\\"location\\\": \\\"San Francisco\\\"\\n}\"\n        }\n      },\n      \"content_filter_results\": {}\n    }\n  ],\n  \"usage\": {\n    \"completion_tokens\": 10,\n    \"prompt_tokens\": 90,\n    \"total_tokens\": 100\n  }\n}\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1692715317359
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define Functions"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytz\n",
        "from datetime import datetime\n",
        "\n",
        "def get_current_time(location):\n",
        "    try:\n",
        "        # Get the timezone for the city\n",
        "        timezone = pytz.timezone(location)\n",
        "\n",
        "        # Get the current time in the timezone\n",
        "        now = datetime.now(timezone)\n",
        "        current_time = now.strftime(\"%I:%M:%S %p\")\n",
        "\n",
        "        return current_time\n",
        "    except:\n",
        "        return \"Sorry, I couldn't find the timezone for that location.\""
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1692715533567
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_current_time(\"America/New_York\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "'10:42:13 AM'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1692715333576
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Call a Function using AOAI"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Describe the functions so that the model knows how to call them"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "functions = [\n",
        "        {\n",
        "            \"name\": \"get_current_time\",\n",
        "            \"description\": \"Get the current time in a given location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The location name. The pytz is used to get the timezone for that location. Location names should be in a format like America/New_York, Asia/Bangkok, Europe/London\",\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"location\"],\n",
        "            },\n",
        "        }\n",
        "    ]\n",
        "\n",
        "available_functions = {\n",
        "            \"get_current_time\": get_current_time\n",
        "        } "
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1692715536546
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Define a helper function to validate the function call"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "\n",
        "# helper method used to check if the correct arguments are provided to a function\n",
        "def check_args(function, args):\n",
        "    sig = inspect.signature(function)\n",
        "    params = sig.parameters\n",
        "\n",
        "    # Check if there are extra arguments\n",
        "    for name in args:\n",
        "        if name not in params:\n",
        "            return False\n",
        "    # Check if the required arguments are provided \n",
        "    for name, param in params.items():\n",
        "        if param.default is param.empty and name not in args:\n",
        "            return False\n",
        "\n",
        "    return True"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1692715539405
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1692715719001
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_conversation(messages, functions, available_functions, deployment_id):\n",
        "    # Step 1: send the conversation and available functions to GPT\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        deployment_id=deployment_id,\n",
        "        messages=messages,\n",
        "        functions=functions,\n",
        "        function_call=\"auto\", \n",
        "    )\n",
        "    response_message = response[\"choices\"][0][\"message\"]\n",
        "\n",
        "\n",
        "    # Step 2: check if GPT wanted to call a function\n",
        "    if response_message.get(\"function_call\"):\n",
        "        print(\"Recommended Function call:\")\n",
        "        print(response_message.get(\"function_call\"))\n",
        "        print()\n",
        "        \n",
        "        # Step 3: call the function\n",
        "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
        "        \n",
        "        function_name = response_message[\"function_call\"][\"name\"]\n",
        "        \n",
        "        # verify function exists\n",
        "        if function_name not in available_functions:\n",
        "            return \"Function \" + function_name + \" does not exist\"\n",
        "        function_to_call = available_functions[function_name]  \n",
        "        \n",
        "        # verify function has correct number of arguments\n",
        "        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
        "        if check_args(function_to_call, function_args) is False:\n",
        "            return \"Invalid number of arguments for function: \" + function_name\n",
        "        function_response = function_to_call(**function_args)\n",
        "        \n",
        "        print(\"Output of function call:\")\n",
        "        print(function_response)\n",
        "        print()\n",
        "        \n",
        "        # Step 4: send the info on the function call and function response to GPT\n",
        "        \n",
        "        # adding assistant response to messages\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": response_message[\"role\"],\n",
        "                \"name\": response_message[\"function_call\"][\"name\"],\n",
        "                \"content\": response_message[\"function_call\"][\"arguments\"],\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # adding function response to messages\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"function\",\n",
        "                \"name\": function_name,\n",
        "                \"content\": function_response,\n",
        "            }\n",
        "        )  # extend conversation with function response\n",
        "\n",
        "        print(\"Messages in second request:\")\n",
        "        for message in messages:\n",
        "            print(message)\n",
        "        print()\n",
        "\n",
        "        second_response = openai.ChatCompletion.create(\n",
        "            messages=messages,\n",
        "            deployment_id=deployment_id\n",
        "        )  # get a new response from GPT where it can see the function response\n",
        "\n",
        "        return second_response"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1692715561419
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{\"role\": \"user\", \"content\": \"What time is it in New York?\"}]\n",
        "assistant_response = run_conversation(messages, functions, available_functions, \"gpt-4-32k\")\n",
        "print(assistant_response['choices'][0]['message'])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Recommended Function call:\n{\n  \"name\": \"get_current_time\",\n  \"arguments\": \"{\\n  \\\"location\\\": \\\"America/New_York\\\"\\n}\"\n}\n\nOutput of function call:\n10:48:45 AM\n\nMessages in second request:\n{'role': 'user', 'content': 'What time is it in New York?'}\n{'role': 'assistant', 'name': 'get_current_time', 'content': '{\\n  \"location\": \"America/New_York\"\\n}'}\n{'role': 'function', 'name': 'get_current_time', 'content': '10:48:45 AM'}\n\n{\n  \"role\": \"assistant\",\n  \"content\": \"The current time in New York is 10:48 AM.\"\n}\n"
        }
      ],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1692715727367
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}