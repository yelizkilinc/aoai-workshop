{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokens"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key topics:\n",
        "\n",
        "Tokens: basic units of text/code for LLM AI models to process/generate language.\n",
        "Tokenization: splitting input/output texts into smaller units for LLM AI models.\n",
        "Vocabulary size: the number of tokens each model uses, which varies among different GPT models.\n",
        "Tokenization cost: affects the memory and computational resources that a model needs, which influences the cost and performance of running an OpenAI or Azure OpenAI model."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Set up Azure OpenAI\n",
        "load_dotenv(\"credentials.env\")\n",
        "\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\") # Api base is the 'Endpoint' which can be found in Azure Portal where Azure OpenAI is created. It looks like https://xxxxxx.openai.azure.com/\n",
        "openai.api_version = \"2023-03-15-preview\"\n",
        "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1692044029992
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\") #GPT2TokenizerFast is a class for tokenizing text using the GPT-2 model. It is based on byte-level Byte-Pair-Encoding and can encode or decode text quickly. \n",
        "prompt = \"The road to creating new medicines and vaccines has traditionally been long and winding!\"\n",
        "tokens = tokenizer(prompt)\n",
        "print('Total number of tokens:', len(tokens['input_ids']))\n",
        "print('Tokens : ', [tokenizer.decode(t) for t in tokens['input_ids']])\n",
        "print(\"Tokens' numerical values:\", tokens['input_ids'])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Total number of tokens: 15\nTokens :  ['The', ' road', ' to', ' creating', ' new', ' medicines', ' and', ' vaccines', ' has', ' traditionally', ' been', ' long', ' and', ' winding', '!']\nTokens' numerical values: [464, 2975, 284, 4441, 649, 23533, 290, 18336, 468, 16083, 587, 890, 290, 28967, 0]\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1692043980369
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install tiktoken #The open source version of tiktoken can be installed from PyPI"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: tiktoken in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (0.3.3)\nRequirement already satisfied: regex>=2022.1.18 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from tiktoken) (2022.10.31)\nRequirement already satisfied: requests>=2.26.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from tiktoken) (2.28.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.0.1)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2022.6.15)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/anaconda/envs/azureml_py310_sdkv2/bin/python -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1692046188868
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken \n",
        "\n",
        "cl100k_base = tiktoken.get_encoding(\"cl100k_base\") \n",
        "\n",
        "enc = tiktoken.Encoding( \n",
        "    name=\"gpt-35-turbo\",  \n",
        "    pat_str=cl100k_base._pat_str, \n",
        "    mergeable_ranks=cl100k_base._mergeable_ranks, \n",
        "    special_tokens={ \n",
        "        **cl100k_base._special_tokens, \n",
        "        \"<|im_start|>\": 100264, \n",
        "        \"<|im_end|>\": 100265\n",
        "    } \n",
        ") \n",
        "\n",
        "tokens = enc.encode( \n",
        "    \"The road to creating new medicines and vaccines has traditionally been long and winding!\"\n",
        ") \n",
        "\n",
        "print('Total number of tokens:', len(tokens))\n",
        "print('Tokens : ', [enc.decode([t]) for t in tokens])\n",
        "print(\"Tokens' numerical values:\", tokens)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<class 'list'>\nTotal number of tokens: 15\nTokens :  ['The', ' road', ' to', ' creating', ' new', ' medicines', ' and', ' vaccines', ' has', ' traditionally', ' been', ' long', ' and', ' winding', '!']\nTokens' numerical values: [791, 5754, 311, 6968, 502, 39653, 323, 40300, 706, 36342, 1027, 1317, 323, 54826, 0]\n"
        }
      ],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1692044367895
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.Completion.create(\n",
        "    engine=\"textdavinci003yk\", #gpt-35-turbo\",\n",
        "    prompt=prompt,\n",
        "    max_tokens=60,\n",
        "    n=2\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1692044457449
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Show 2 returned results"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print('='*30, 'ANSWER #1', '='*30)\n",
        "print(response['choices'][0]['text'])\n",
        "print('='*30, 'ANSWER #2', '='*30)\n",
        "print(response['choices'][1]['text'])\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "============================== ANSWER #1 ==============================\n\n\nIn order to create new medicines and vaccines, pharmaceutical companies must first conduct extensive research and development before they can submit their drug or vaccine candidate to the FDA for approval. This process can often take years of intensive work and often includes a wealth of specific steps. \n\nThe process usually starts\n============================== ANSWER #2 ==============================\n In general, it takes approximately 12 to 15 years on average to develop an approved prescription medicine, and this process includes four distinct phases of drug development.\n\nPhase 1: This phase typically lasts approximately 6 - 12 months and involves very small groups of people (20 - 80) receiving the drug for\n"
        }
      ],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1692044459849
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usage"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 31,
          "data": {
            "text/plain": "<OpenAIObject text_completion id=cmpl-7nYLYxzpT8w2dxxZn4hMKVjBGK7dX at 0x7f2bdc6aafc0> JSON: {\n  \"id\": \"cmpl-7nYLYxzpT8w2dxxZn4hMKVjBGK7dX\",\n  \"object\": \"text_completion\",\n  \"created\": 1692044456,\n  \"model\": \"text-davinci-003\",\n  \"choices\": [\n    {\n      \"text\": \"\\n\\nIn order to create new medicines and vaccines, pharmaceutical companies must first conduct extensive research and development before they can submit their drug or vaccine candidate to the FDA for approval. This process can often take years of intensive work and often includes a wealth of specific steps. \\n\\nThe process usually starts\",\n      \"index\": 0,\n      \"finish_reason\": \"length\",\n      \"logprobs\": null\n    },\n    {\n      \"text\": \" In general, it takes approximately 12 to 15 years on average to develop an approved prescription medicine, and this process includes four distinct phases of drug development.\\n\\nPhase 1: This phase typically lasts approximately 6 - 12 months and involves very small groups of people (20 - 80) receiving the drug for\",\n      \"index\": 1,\n      \"finish_reason\": \"length\",\n      \"logprobs\": null\n    }\n  ],\n  \"usage\": {\n    \"completion_tokens\": 120,\n    \"prompt_tokens\": 15,\n    \"total_tokens\": 135\n  }\n}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1692044462998
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response['usage']"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "<OpenAIObject at 0x7f2385199c10> JSON: {\n  \"completion_tokens\": 120,\n  \"prompt_tokens\": 15,\n  \"total_tokens\": 135\n}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1692023692102
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of splitting the input and output texts into smaller units that can be processed by the LLM AI models. Tokens can be words, characters, subwords, or symbols, depending on the type and the size of the model. Tokenization can help the model to handle different languages, vocabularies, and formats, and to reduce the computational and memory costs. Tokenization can also affect the quality and the diversity of the generated texts, by influencing the meaning and the context of the tokens. Tokenization can be done using different methods, such as rule-based, statistical, or neural, depending on the complexity and the variability of the texts.\n",
        "\n",
        "OpenAI and Azure OpenAI uses a subword tokenization method called \"Byte-Pair Encoding (BPE)\" for its GPT-based models. BPE is a method that merges the most frequently occurring pairs of characters or bytes into a single token, until a certain number of tokens or a vocabulary size is reached. BPE can help the model to handle rare or unseen words, and to create more compact and consistent representations of the texts. BPE can also allow the model to generate new words or tokens, by combining existing ones. The way that tokenization is different dependent upon the different model Ada, Babbage, Curie, and Davinci is mainly based on the number of tokens or the vocabulary size that each model uses. Ada has the smallest vocabulary size, with 50,000 tokens, and Davinci has the largest vocabulary size, with 60,000 tokens. Babbage and Curie have the same vocabulary size, with 57,000 tokens. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/tokens"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2139c70ac98f3202d028164a545621647e07f47fd6f5d8ac55cf952bf7c15ed1"
      }
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}